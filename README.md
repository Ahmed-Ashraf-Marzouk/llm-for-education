# llm-for-education
You can our presentation for the demo here [link](https://docs.google.com/presentation/d/1bWzzKJ3W2_BnhzZU4grf8za71GtWf_QWANrAvfvugvE/edit?usp=sharing)!
# ğŸ§  LLM for Education

**LLM for Education** is a research-focused project that explores fine-tuning and aligning large language models (LLMs) for educational applications. It combines cutting-edge techniques like LoRA/QLoRA, Direct Preference Optimization (DPO), reward modeling, and quantization to build domain-specific educational agents.

---

## Features

- ğŸ”§ Fine-tuning with LoRA/QLoRA using `peft`
- ğŸ§‘â€ğŸ« Instruction tuning and alignment with DPO
- âš™ï¸ Quantization support via `autoawq`, `gguf`, and `optimum`
- ğŸ“ˆ Experiment tracking with `wandb`, `TensorBoard`
- ğŸ““ Notebook-based workflows with `qwen.ipynb`

---

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Ahmed-Ashraf-Marzouk/llm-for-education.git
cd llm-for-education
```

### 2. Create a Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### Contributing
We welcome contributions from researchers, developers, and educators.

1. Fork the repository

1. Create a feature branch

1. Commit your changes

1. Open a pull request

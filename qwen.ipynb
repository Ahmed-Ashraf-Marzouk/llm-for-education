{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig # Import for 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"qwen2-1.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # Load model weights in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",        # Use NF4 quantization for better performance\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Compute activations in float16 for speed\n",
    "    bnb_4bit_use_double_quant=True,   # Optional: further quantize the quantization constants\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",             \n",
    "    torch_dtype=torch.float16,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=12,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_token\"], # Ensure these are trained in full precision\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"question\", \"hint\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_prompt(example):\n",
    "    # Ensure this matches the tokenizer's chat template or your desired format precisely.\n",
    "    # For Qwen2-Instruct, it expects a specific chat template.\n",
    "    # Let's use the tokenizer's apply_chat_template for consistency.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['hint']}\n",
    "    ]\n",
    "    # apply_chat_template will convert messages into the correct format for the model\n",
    "    # It also handles adding special tokens like <|im_start|> and <|im_end|>\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Keep only 'text' column\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=f\"{model_name}-SFT\",\n",
    "    per_device_train_batch_size=1,      # **CRITICAL**: Reduce this to 1 to start\n",
    "    gradient_accumulation_steps=8,      # **CRITICAL**: Increase this to compensate for batch_size=1\n",
    "    gradient_checkpointing=True,        # **CRITICAL**: Saves memory by recomputing activations\n",
    "    learning_rate=2e-4,                 # Common learning rate for LoRA fine-tuning\n",
    "    num_train_epochs=12,                 # Or more, depending on dataset size and convergence\n",
    "    optim=\"paged_adamw_8bit\",           # Use 8-bit AdamW optimizer for memory savings\n",
    "    logging_steps=10,                   # Log progress frequently\n",
    "    save_steps=500,                     # Save checkpoints\n",
    "    fp16=True,                          # Enable mixed precision training (PyTorch native)\n",
    "    # SFTConfig specific parameters:\n",
    "    dataset_text_field=\"text\",          # The name of the column containing the text\n",
    "    packing=True,                       # Packs multiple short examples into one longer sequence\n",
    "    max_seq_length=1024,                # Adjust based on your data and memory. Shorter sequences save memory.\n",
    "    # We pass peft_config directly to SFTTrainer, not here.\n",
    ")\n",
    "\n",
    "# -------------------- SFTTrainer Initialization and Training --------------------\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,        # Pass the SFTConfig object here\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,   # Pass your LoRA config here\n",
    "    # The 'packing' and 'max_seq_length' are now part of SFTConfig and automatically handled by SFTTrainer\n",
    "    # No need for formatting_prompts_func directly as the dataset is already prepared with 'text' column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 08:32, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import DPOConfig, DPOTrainer # Import DPOConfig and DPOTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"merged_qwen2-1.5b_SFT_model\" # Use the instruct version, as DPO optimizes for instruction following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig for 4-bit quantization (memory efficiency)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Add pad token if missing, crucial for batching and generation\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\.nlp\\Lib\\site-packages\\transformers\\quantizers\\auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the Policy Model (the one to be fine-tuned)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Auto-distribute model across available GPUs\n",
    "    torch_dtype=torch.float16, # Use float16 for computation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reference Model. This is usually the SFT-tuned model or the base model.\n",
    "# For memory efficiency, load it with the same 4-bit quantization.\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize token embeddings for both models if a new pad_token was added\n",
    "if tokenizer.pad_token is not None and len(tokenizer) > model.config.vocab_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    ref_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Prepare models for k-bit training (PEFT compatibility)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "ref_model = prepare_model_for_kbit_training(ref_model) # Also prepare ref_model for kbit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration (PEFT)\n",
    "peft_config = LoraConfig(\n",
    "    r=12,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\", # Apply LoRA to all linear layers\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"], # Ensure these key layers are trained in full precision\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PEFT to the policy model\n",
    "model = get_peft_model(model, peft_config)\n",
    "# The ref_model does not get PEFT applied for DPO, it remains frozen and serves as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DPO Dataset Preparation ---\n",
    "# Load your dataset with 'question', 'hint' (chosen), and 'response' (rejected)\n",
    "raw_dataset = load_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n",
    "\n",
    "def format_dpo_examples(example):\n",
    "    \"\"\"\n",
    "    Formats the raw dataset example into the 'prompt', 'chosen', 'rejected'\n",
    "    format required by DPOTrainer, using the Qwen chat template.\n",
    "    \"\"\"\n",
    "    # Construct the user prompt using the Qwen chat template\n",
    "    user_messages = [{\"role\": \"user\", \"content\": example['question']}]\n",
    "    prompt_text = tokenizer.apply_chat_template(user_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Construct the chosen response, including the prompt and assistant's turn\n",
    "    chosen_messages = user_messages + [{\"role\": \"assistant\", \"content\": example['hint']}]\n",
    "    chosen_text = tokenizer.apply_chat_template(chosen_messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    # Construct the rejected response, including the prompt and assistant's turn\n",
    "    rejected_messages = user_messages + [{\"role\": \"assistant\", \"content\": example['response']}]\n",
    "    rejected_text = tokenizer.apply_chat_template(rejected_messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"chosen\": chosen_text,\n",
    "        \"rejected\": rejected_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to your dataset\n",
    "dpo_dataset = raw_dataset.map(\n",
    "    format_dpo_examples,\n",
    "    remove_columns=raw_dataset.column_names # Remove original columns after mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DPO Training Arguments (DPOConfig) ---\n",
    "dpo_training_args = DPOConfig(\n",
    "    output_dir=f\"{model_name}-DPO_2\",\n",
    "    per_device_train_batch_size=1,        # **CRITICAL**: Keep at 1 for memory\n",
    "    gradient_accumulation_steps=8,        # **CRITICAL**: Compensate for batch_size=1\n",
    "    gradient_checkpointing=True,          # **CRITICAL**: Saves memory by recomputing activations\n",
    "    learning_rate=5e-5,                   # DPO typically uses a lower LR than SFT\n",
    "    num_train_epochs=3,                   # Adjust based on dataset size and convergence\n",
    "    optim=\"paged_adamw_8bit\",             # Use 8-bit AdamW for memory savings\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=True,                            # Enable mixed precision training\n",
    "    # DPO specific parameters:\n",
    "    beta=0.1,                             # Controls the strength of the preference. Start with 0.1-0.5.\n",
    "    max_length=1024,                      # Max total sequence length for chosen/rejected\n",
    "    max_prompt_length=512,                # Max length for the prompt part\n",
    "    # max_completion_length is automatically derived from max_length and max_prompt_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\.nlp\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\.nlp\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# --- DPOTrainer Initialization and Training ---\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,                  # The frozen reference model\n",
    "    args=dpo_training_args,               # Pass the DPOConfig object\n",
    "    train_dataset=dpo_dataset,\n",
    "    peft_config=peft_config,    \n",
    "    processing_class=tokenizer          # Apply LoRA to the policy model during DPO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting DPO training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: a-a-elghawas (a-a-elghawas-king-fahd-university-of-petroleum-minerals) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\wandb\\run-20250523_125720-qe83wkb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface/runs/qe83wkb4' target=\"_blank\">merged_qwen2-1.5b_SFT_model-DPO_2</a></strong> to <a href='https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface' target=\"_blank\">https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface/runs/qe83wkb4' target=\"_blank\">https://wandb.ai/a-a-elghawas-king-fahd-university-of-petroleum-minerals/huggingface/runs/qe83wkb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\.nlp\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 29:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting DPO training ---\")\n",
    "dpo_trainer.train()\n",
    "\n",
    "print(\"DPO fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: qwen2-1.5b\n",
      "Loading LoRA adapters from: qwen2-1.5b-SFT\n",
      "Merging LoRA adapters into the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\g202411740\\Desktop\\NLP-Project\\llm-for-education\\.nlp\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapters merged.\n",
      "Saving merged model to: ./merged_qwen2-1.5b_SFT_model\n",
      "Merged model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Set the base model name (the one you fine-tuned from)\n",
    "base_model_name = \"qwen2-1.5b\"\n",
    "# Set the directory where your LoRA adapters were saved\n",
    "lora_adapter_path = f\"{base_model_name}-{t}\"\n",
    "\n",
    "# --- 1. Load the Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# Ensure pad_token is set if it was added during training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "# --- 2. Load the Base Model (with quantization if used during training) ---\n",
    "# If you trained with 4-bit quantization, you must load the base model with it too.\n",
    "# Otherwise, load it in full precision (float16 or float32)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config, # Use the same quantization config as training\n",
    "    device_map=\"auto\",              # Load the base model onto available devices\n",
    "    torch_dtype=torch.float16,      # Match compute dtype from training\n",
    ")\n",
    "\n",
    "# --- 3. Load LoRA Adapters ---\n",
    "print(f\"Loading LoRA adapters from: {lora_adapter_path}\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "\n",
    "# --- 4. Merge LoRA Adapters into the Base Model (Optional but recommended for inference) ---\n",
    "# Merging makes the model a regular Hugging Face model, no longer needing PEFT internally.\n",
    "# This often results in faster inference and can be saved as a standard model.\n",
    "print(\"Merging LoRA adapters into the base model...\")\n",
    "model = model.merge_and_unload() # This will put the model in float16 (or original precision)\n",
    "print(\"Adapters merged.\")\n",
    "\n",
    "# --- 5. (Optional) Save the Merged Model ---\n",
    "# This allows you to load the model directly without PEFT in the future.\n",
    "merged_model_output_path = f\"./merged_{base_model_name}_{t}_model\"\n",
    "print(f\"Saving merged model to: {merged_model_output_path}\")\n",
    "model.save_pretrained(merged_model_output_path)\n",
    "tokenizer.save_pretrained(merged_model_output_path)\n",
    "print(\"Merged model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: qwen2-1.5b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Set the base model name (the one you fine-tuned from)\n",
    "base_model_name = \"qwen2-1.5b\"\n",
    "# Set the directory where your LoRA adapters were saved\n",
    "# lora_adapter_path = f\"{model_name}-{t}\"\n",
    "\n",
    "# --- 1. Load the Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# Ensure pad_token is set if it was added during training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "# --- 2. Load the Base Model (with quantization if used during training) ---\n",
    "# If you trained with 4-bit quantization, you must load the base model with it too.\n",
    "# Otherwise, load it in full precision (float16 or float32)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config, # Use the same quantization config as training\n",
    "    device_map=\"auto\",              # Load the base model onto available devices\n",
    "    torch_dtype=torch.float16,      # Match compute dtype from training\n",
    ")\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Inference ---\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# --- 7. Example Inference ---\n",
    "print(\"\\n--- Performing Inference ---\")\n",
    "\n",
    "# Define your prompt (use the same format as your training data)\n",
    "def format_prompt_for_inference(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"} # Assistant's turn to respond\n",
    "    ]\n",
    "    # Use add_generation_prompt=True for inference to prime the model\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_question = \"\"\"\n",
    "اختر الإجابة الصحيحة مما يلي:\n",
    "ما هو جمع العدد 6 + 8\n",
    "    1أ. 4\n",
    "ب. 6\n",
    "ج. 7\n",
    "د. 2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "single_question = \"\"\"\n",
    "اختر الإجابة الصحيحة مما يلي:\n",
    "\"ما هو العدد الفردي التالي للعدد 49؟\n",
    "\n",
    "أ. 48\n",
    "ب. 50\n",
    "ج. 51\n",
    "د. 52\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question: \n",
      "اختر الإجابة الصحيحة مما يلي:\n",
      "\"إذا بدأت رحلتك في الساعة 8:30 صباحاً ووصلت وجهتك في الساعة 10:00 صباحاً، فكم استغرقت رحلتك؟\n",
      "\n",
      "أ. ساعة واحدة\n",
      "ب. ساعة ونصف\n",
      "ج. ساعتان\n",
      "د. نصف ساعة\"\n",
      " ---\n",
      "Response:\n",
      "The answer is \"د. نصف ساعة\" or \"half an hour\". This is because the passage states that the person started their journey at 8:30 a.m. and arrived at their destination at 10:00 a.m., which is a total of 1 hour and 30 minutes. Therefore, the travel time is 1.5 hours, or 90 minutes, not 60 minutes. This means the journey lasted for 30 minutes, which is half an hour. \n",
      "\n",
      "So, the answer is \"د. نصف ساعة\" or \"half an hour\". \n",
      "\n",
      "Explanation: \n",
      "\n",
      "The passage states that the person started their journey at 8:30 a.m. and arrived at their destination at 10:00 a.m. This is a total of 1 hour and 30 minutes. Therefore, the travel time is 1.5 hours, or 90 minutes. This means the journey lasted for 30 minutes, which is half an hour. Therefore, the answer is \"د. نصف ساعة\" or \"half an hour\". \n",
      "\n",
      "Explanation: \n",
      "\n",
      "The passage states that the person started their journey at 8:30 a\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = format_prompt_for_inference(single_question)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# --- 8. Generate response ---\n",
    "print(f\"\\n--- Question: {single_question} ---\")\n",
    "with torch.no_grad(): # Disable gradient calculations for inference to save memory and speed up\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=250, # Max tokens for the generated response\n",
    "        do_sample=True,     # Use sampling for more creative/diverse outputs\n",
    "        temperature=0.7,    # Lower values make output more deterministic (0.7 is a good balance)\n",
    "        top_p=0.9,          # Nucleus sampling\n",
    "        eos_token_id=tokenizer.eos_token_id, # Stop generation at end of sequence token\n",
    "        pad_token_id=tokenizer.pad_token_id  # Important for batch generation (even with batch_size=1)\n",
    "    )\n",
    "\n",
    "# --- 9. Decode the generated tokens ---\n",
    "# Skip the prompt tokens to get only the generated response\n",
    "response = tokenizer.decode(outputs[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response:\\n{response}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
